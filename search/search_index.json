{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dataframe-io \u00b6 Read and write dataframes anywhere Free software: Apache-2.0 Documentation: https://chr1st1ank.github.io/dataframe-io/ Features \u00b6 TODO","title":"Home"},{"location":"#dataframe-io","text":"Read and write dataframes anywhere Free software: Apache-2.0 Documentation: https://chr1st1ank.github.io/dataframe-io/","title":"dataframe-io"},{"location":"#features","text":"TODO","title":"Features"},{"location":"authors/","text":"Credits \u00b6 Creator \u00b6 Christian Krudewig chr1st1ank@krudewig-online.de Contributors \u00b6 None yet. Why not be the first?","title":"Authors"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#creator","text":"Christian Krudewig chr1st1ank@krudewig-online.de","title":"Creator"},{"location":"authors/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/chr1st1ank/dframeio/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 dataframe-io could always use more documentation, whether as part of the official dataframe-io docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dframeio/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up dframeio for local development. Fork the dframeio repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dframeio.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dframeio/actions and make sure that the tests pass for all supported Python versions. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Github actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/chr1st1ank/dframeio/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"dataframe-io could always use more documentation, whether as part of the official dataframe-io docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dframeio/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up dframeio for local development. Fork the dframeio repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dframeio.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dframeio/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Github actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"history/","text":"History \u00b6 0.1.0 (2021-05-23) \u00b6 First release on PyPI.","title":"History"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#010-2021-05-23","text":"First release on PyPI.","title":"0.1.0 (2021-05-23)"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install dataframe-io, run this command in your terminal: $ pip install dframeio This is the preferred method to install dataframe-io, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for dataframe-io can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dframeio Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dframeio/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install dataframe-io, run this command in your terminal: $ pip install dframeio This is the preferred method to install dataframe-io, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for dataframe-io can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dframeio Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dframeio/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use dataframe-io in a project import dframeio","title":"Usage"},{"location":"usage/#usage","text":"To use dataframe-io in a project import dframeio","title":"Usage"},{"location":"api/dframeio.abstract/","text":"module dframeio. abstract </> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </> class dframeio.abstract. AbstractDataFrameReader ( ) </> Interface for reading dataframes from different storage drivers Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read data into a dict of named columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read data into a pandas.DataFrame </> abstract method read_to_pandas ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read data into a pandas.DataFrame Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. The filter and limit arguments are applied in the following order: first the row_filter expression is applied and all matching rows go into the next step, afterwards the limit argument is applied if given, in the next step the sample argument is applied if it is specified, at the very end drop_duplicates takes effect. This means that this flag may reduce the output size further and that fewer rows may be returned as specified with limit or sample if there are duplicates in the data. abstract method read_to_dict ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read data into a dict of named columns Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for read_to_pandas() . class dframeio.abstract. AbstractDataFrameWriter ( ) </> Interface for writing dataframes to different storage drivers Methods write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> abstract method write_replace ( target , dataframe ) </> Write data with full replacement of an existing dataset abstract method write_append ( target , dataframe ) </> Write data in append-mode","title":"dframeio.abstract"},{"location":"api/dframeio.abstract/#dframeioabstract","text":"</> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </> class","title":"dframeio.abstract"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframereader","text":"</> Interface for reading dataframes from different storage drivers Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read data into a dict of named columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read data into a pandas.DataFrame </> abstract method","title":"dframeio.abstract.AbstractDataFrameReader"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframereaderread_to_pandas","text":"</> Read data into a pandas.DataFrame Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. The filter and limit arguments are applied in the following order: first the row_filter expression is applied and all matching rows go into the next step, afterwards the limit argument is applied if given, in the next step the sample argument is applied if it is specified, at the very end drop_duplicates takes effect. This means that this flag may reduce the output size further and that fewer rows may be returned as specified with limit or sample if there are duplicates in the data. abstract method","title":"dframeio.abstract.AbstractDataFrameReader.read_to_pandas"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframereaderread_to_dict","text":"</> Read data into a dict of named columns Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for read_to_pandas() . class","title":"dframeio.abstract.AbstractDataFrameReader.read_to_dict"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframewriter","text":"</> Interface for writing dataframes to different storage drivers Methods write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> abstract method","title":"dframeio.abstract.AbstractDataFrameWriter"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframewriterwrite_replace","text":"</> Write data with full replacement of an existing dataset abstract method","title":"dframeio.abstract.AbstractDataFrameWriter.write_replace"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframewriterwrite_append","text":"</> Write data in append-mode","title":"dframeio.abstract.AbstractDataFrameWriter.write_append"},{"location":"api/dframeio/","text":"package dframeio </> Top-level package for dataframe-io. module dframeio. parquet </> Implementation to access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> module dframeio. abstract </> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </>","title":"dframeio"},{"location":"api/dframeio/#dframeio","text":"</> Top-level package for dataframe-io. module","title":"dframeio"},{"location":"api/dframeio/#dframeioparquet","text":"</> Implementation to access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> module","title":"dframeio.parquet"},{"location":"api/dframeio/#dframeioabstract","text":"</> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </>","title":"dframeio.abstract"},{"location":"api/dframeio.parquet/","text":"module dframeio. parquet </> Implementation to access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> class dframeio.parquet. ParquetBackend ( base_path , partitions=None , rows_per_file=0 ) </> Bases dframeio.abstract.AbstractDataFrameReader dframeio.abstract.AbstractDataFrameWriter Backend to read and write parquet datasets Parameters partitions (iterable of str, optional) \u2014 (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: column_name=value . Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file (int, optional) \u2014 (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises TypeError \u2014 If any of the input arguments has a diffent type as documented ValueError \u2014 If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read a parquet dataset from disk into a dictionary of columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read a parquet dataset from disk into a pandas DataFrame </> write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> method read_to_pandas ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read a parquet dataset from disk into a pandas DataFrame Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. Raises ValueError \u2014 If path specified with source is outside of the base path The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method read_to_dict ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read a parquet dataset from disk into a dictionary of columns Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values Raises NotImplementedError \u2014 If row_filter is given, because this is not yet implemented The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method write_replace ( target , dataframe ) </> Write data with full replacement of an existing dataset Parameters target (str) \u2014 The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the __init__() function. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises ValueError \u2014 If the dataframe does not contain the columns to partition by as specified in the __init__() function. method write_append ( target , dataframe ) </> Write data in append-mode","title":"dframeio.parquet"},{"location":"api/dframeio.parquet/#dframeioparquet","text":"</> Implementation to access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> class","title":"dframeio.parquet"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackend","text":"</> Bases dframeio.abstract.AbstractDataFrameReader dframeio.abstract.AbstractDataFrameWriter Backend to read and write parquet datasets Parameters partitions (iterable of str, optional) \u2014 (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: column_name=value . Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file (int, optional) \u2014 (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises TypeError \u2014 If any of the input arguments has a diffent type as documented ValueError \u2014 If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read a parquet dataset from disk into a dictionary of columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read a parquet dataset from disk into a pandas DataFrame </> write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> method","title":"dframeio.parquet.ParquetBackend"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendread_to_pandas","text":"</> Read a parquet dataset from disk into a pandas DataFrame Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. Raises ValueError \u2014 If path specified with source is outside of the base path The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method","title":"dframeio.parquet.ParquetBackend.read_to_pandas"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendread_to_dict","text":"</> Read a parquet dataset from disk into a dictionary of columns Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values Raises NotImplementedError \u2014 If row_filter is given, because this is not yet implemented The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method","title":"dframeio.parquet.ParquetBackend.read_to_dict"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendwrite_replace","text":"</> Write data with full replacement of an existing dataset Parameters target (str) \u2014 The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the __init__() function. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises ValueError \u2014 If the dataframe does not contain the columns to partition by as specified in the __init__() function. method","title":"dframeio.parquet.ParquetBackend.write_replace"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendwrite_append","text":"</> Write data in append-mode","title":"dframeio.parquet.ParquetBackend.write_append"},{"location":"api/source/dframeio.abstract/","text":"SOURCE CODE dframeio. abstract DOCS \"\"\"Abstract interfaces for all storage backends\"\"\" from abc import abstractmethod from typing import Dict , List , Union try : import pandas as pd except ImportError : pd = None class AbstractDataFrameReader : DOCS \"\"\"Interface for reading dataframes from different storage drivers\"\"\" @abstractmethod DOCS def read_to_pandas ( self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> pd . DataFrame : \"\"\"Read data into a pandas.DataFrame Args: source: A string specifying the data source (format differs by backend) columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows. limit: Maximum number of rows to return (top-n) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows from the final selection Returns: A pandas DataFrame with the requested data. The filter and limit arguments are applied in the following order: - first the `row_filter` expression is applied and all matching rows go into the next step, - afterwards the `limit` argument is applied if given, - in the next step the `sample` argument is applied if it is specified, - at the very end `drop_duplicates` takes effect. This means that this flag may reduce the output size further and that fewer rows may be returned as specified with `limit` or `sample` if there are duplicates in the data. \"\"\" raise NotImplementedError () @abstractmethod DOCS def read_to_dict ( self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> Dict [ str , List ]: \"\"\"Read data into a dict of named columns Args: source: A string specifying the data source (format differs by backend) columns: List of column names to limit the reading to row_filter: NOT IMPLEMENTED. Reserved keyword for filtering rows. limit: Maximum number of rows to return (top-n) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows Returns: A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for [`read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" raise NotImplementedError () class AbstractDataFrameWriter : DOCS \"\"\"Interface for writing dataframes to different storage drivers\"\"\" @abstractmethod DOCS def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data with full replacement of an existing dataset\"\"\" raise NotImplementedError () @abstractmethod DOCS def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data in append-mode\"\"\" raise NotImplementedError ()","title":"dframeio.abstract"},{"location":"api/source/dframeio/","text":"SOURCE CODE dframeio DOCS \"\"\"Top-level package for dataframe-io.\"\"\" __version__ = \"0.1.0\" # Add Backends one by one if dependencies are available backends = [] try : from dframeio.parquet import ParquetBackend backends . append ( ParquetBackend ) except ModuleNotFoundError as e : if e . name == \"pyarrow\" : pass else : raise","title":"dframeio"},{"location":"api/source/dframeio.parquet/","text":"SOURCE CODE dframeio. parquet DOCS \"\"\"Implementation to access parquet datasets using pyarrow.\"\"\" import collections import shutil from pathlib import Path from typing import Dict , Iterable , List , Union from .abstract import AbstractDataFrameReader , AbstractDataFrameWriter try : import pandas as pd except ImportError : pd = None try : import pyarrow as pa import pyarrow.parquet as pq except ModuleNotFoundError as e : if e . name == \"pyarrow\" : pq = None else : raise class ParquetBackend ( AbstractDataFrameReader , AbstractDataFrameWriter ): DOCS \"\"\"Backend to read and write parquet datasets Args: base_path: partitions: (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: `column_name=value`. Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file: (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises: ValueError: If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. TypeError: If any of the input arguments has a diffent type as documented \"\"\" def __init__ ( self , base_path : str , partitions : Iterable [ str ] = None , rows_per_file : int = 0 ): self . _base_path = base_path if partitions is not None and rows_per_file != 0 : raise ValueError ( \"Only one of 'partitions' and 'rows_per_file' can be used.\" ) if rows_per_file != 0 : if not isinstance ( rows_per_file , int ): raise TypeError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if rows_per_file < 0 : raise ValueError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if partitions is not None : if isinstance ( partitions , ( str , bytes )): raise TypeError ( \"partitions must be an integer or an iterable of column names\" ) for _ in partitions : # Raises TypeError if not iterable break self . _partitions = partitions self . _rows_per_file = rows_per_file def read_to_pandas ( DOCS self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> pd . DataFrame : \"\"\"Read a parquet dataset from disk into a pandas DataFrame Args: source: The path of the file or folder with a parquet dataset to read columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows. limit: Maximum number of rows to return (top-n) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows from the final selection Returns: A pandas DataFrame with the requested data. Raises: ValueError: If path specified with `source` is outside of the base path The logic of the filtering arguments is as documented for [`AbstractDataFrameReader.read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" full_path = Path ( self . _base_path ) / source if Path ( self . _base_path ) not in full_path . parents : raise ValueError ( f \"The given source path { source } is not in base_path { self . _base_path } !\" ) # TODO: use read_pandas() # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_pandas.html#pyarrow.parquet.read_pandas df = pq . read_table ( str ( full_path ), columns = columns , use_threads = True , use_pandas_metadata = True ) . to_pandas () if row_filter : df = df . query ( row_filter ) if limit > 0 : df = df . head ( limit ) if sample > 0 : df = df . sample ( sample ) return df def read_to_dict ( DOCS self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> Dict [ str , List ]: \"\"\"Read a parquet dataset from disk into a dictionary of columns Args: source: The path of the file or folder with a parquet dataset to read columns: List of column names to limit the reading to row_filter: NOT IMPLEMENTED. Reserved keyword for filtering rows. limit: Maximum number of rows to return (top-n) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows Returns: A dictionary with column names as key and a list with column values as values Raises: NotImplementedError: If row_filter is given, because this is not yet implemented The logic of the filtering arguments is as documented for [`AbstractDataFrameReader.read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" full_path = self . _validated_full_path ( source ) df = pq . read_table ( str ( full_path ), columns = columns , use_threads = True , use_pandas_metadata = True ) . to_pydict () if row_filter : # TODO: Pyarrow supports filtering on loading # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html raise NotImplementedError ( \"Row filtering is not implemented for dicts\" ) return df def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): DOCS \"\"\"Write data with full replacement of an existing dataset Args: target: The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the [`__init__()`](dframeio.parquet.ParquetBackend) function. dataframe: The data to write as pandas.DataFrame or as a Python dictionary in the format `column_name: [column_data]` Raises: ValueError: If the dataframe does not contain the columns to partition by as specified in the [`__init__()`](dframeio.parquet.ParquetBackend) function. \"\"\" full_path = self . _validated_full_path ( target ) if full_path . exists (): if full_path . is_file (): full_path . unlink () elif full_path . is_dir (): shutil . rmtree ( str ( full_path ), ignore_errors = True ) if self . _rows_per_file > 0 : full_path . mkdir ( exist_ok = True ) for i in range ( 0 , self . _n_rows ( dataframe ), self . _rows_per_file ): pq . write_table ( self . _dataframe_slice_as_arrow_table ( dataframe , i , i + self . _rows_per_file ), where = str ( full_path / ( full_path . name + str ( i ))), flavor = \"spark\" , compression = \"snappy\" , ) else : arrow_table = self . _to_arrow_table ( dataframe ) if self . _partitions is not None : missing_columns = set ( self . _partitions ) - set ( arrow_table . column_names ) if missing_columns : raise ValueError ( f \"Expected the dataframe to have the partition columns { missing_columns } \" ) pq . write_to_dataset ( arrow_table , root_path = str ( full_path ), partition_cols = self . _partitions , flavor = \"spark\" , compression = \"snappy\" , ) else : pq . write_table ( arrow_table , where = str ( full_path ), flavor = \"spark\" , compression = \"snappy\" , ) def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): DOCS \"\"\"Write data in append-mode\"\"\" # TODO: Implement raise NotImplementedError () @staticmethod def _n_rows ( dataframe ): if isinstance ( dataframe , pd . DataFrame ): return len ( dataframe ) if isinstance ( dataframe , collections . Mapping ): return len ( next ( iter ( dataframe . values ()))) raise ValueError ( \"dataframe must be a pandas.DataFrame or dict\" ) @staticmethod def _dataframe_slice_as_arrow_table ( dataframe : Union [ pd . DataFrame , Dict [ str , List ]], start : int , stop : int ): if isinstance ( dataframe , pd . DataFrame ): return pa . Table . from_pandas ( dataframe . iloc [ start : stop ], preserve_index = True ) if isinstance ( dataframe , collections . Mapping ): return pa . Table . from_pydict ( { colname : col [ start : stop ] for colname , col in dataframe . items ()} ) raise ValueError ( \"dataframe must be a pandas.DataFrame or dict\" ) def _validated_full_path ( self , path : Union [ str , Path ]) -> Path : \"\"\"Make sure the given path is in self._base_path and return the full path Returns: The full path as pathlib object Raises: ValueError: If the path is not in the base path \"\"\" full_path = Path ( self . _base_path ) / path if Path ( self . _base_path ) not in full_path . parents : raise ValueError ( f \"The given path { path } is not in base_path { self . _base_path } !\" ) return full_path @staticmethod def _to_arrow_table ( dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Convert the dataframe to an arrow table\"\"\" if isinstance ( dataframe , pd . DataFrame ): return pa . Table . from_pandas ( dataframe , preserve_index = True ) if isinstance ( dataframe , collections . Mapping ): return pa . Table . from_pydict ( dataframe ) raise ValueError ( \"dataframe must be a pandas.DataFrame or dict\" )","title":"dframeio.parquet"}]}