{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dataframe-io \u00b6 Read and write dataframes anywhere Free software: Apache-2.0 Documentation: https://dframeio.readthedocs.io Features \u00b6 TODO Credits \u00b6 This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template.","title":"Home"},{"location":"#dataframe-io","text":"Read and write dataframes anywhere Free software: Apache-2.0 Documentation: https://dframeio.readthedocs.io","title":"dataframe-io"},{"location":"#features","text":"TODO","title":"Features"},{"location":"#credits","text":"This package was created with Cookiecutter and the zillionare/cookiecutter-pypackage project template.","title":"Credits"},{"location":"api/","text":"\u00b6 Top-level package for dataframe-io. abstract \u00b6 Abstract interfaces for all storage backends AbstractDataFrameReader \u00b6 Interface for reading dataframes from different storage drivers read_to_dict ( self , source , columns = None , row_filter = None , drop_duplicates = False , limit =- 1 , sample =- 1 ) \u00b6 Read data into a dict of named columns Source code in dframeio/abstract.py @abstractmethod def read_to_dict ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> Dict [ str , List ]: \"\"\"Read data into a dict of named columns\"\"\" raise NotImplementedError () read_to_pandas ( self , source , columns = None , row_filter = None , drop_duplicates = False , limit =- 1 , sample =- 1 ) \u00b6 Read data into a pandas.DataFrame Source code in dframeio/abstract.py @abstractmethod def read_to_pandas ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> pd . DataFrame : \"\"\"Read data into a pandas.DataFrame\"\"\" raise NotImplementedError () AbstractDataFrameWriter \u00b6 Interface for writing dataframes to different storage drivers write_append ( self , target , dataframe ) \u00b6 Write data in append-mode Source code in dframeio/abstract.py @abstractmethod def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data in append-mode\"\"\" raise NotImplementedError () write_replace ( self , target , dataframe ) \u00b6 Write data with full replacement of an existing dataset Source code in dframeio/abstract.py @abstractmethod def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data with full replacement of an existing dataset\"\"\" raise NotImplementedError () parquet \u00b6 Implementation to access parquet datasets using pyarrow. ParquetBackend \u00b6 Backend to read and write parquet datasets __init__ ( self , base_path , partitions = None , rows_per_file = 0 ) special \u00b6 Create a new ParquetBackend object Parameters: Name Type Description Default base_path str required partitions Iterable[str] (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: column_name=value . Per default data is written as a single file. Cannot be combined with rows_per_file. None rows_per_file int (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. 0 Exceptions: Type Description ValueError If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. TypeError If any of the input arguments has a diffent type as documented Source code in dframeio/parquet.py def __init__ ( self , base_path : str , partitions : Iterable [ str ] = None , rows_per_file : int = 0 ): \"\"\"Create a new ParquetBackend object Args: base_path: partitions: (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: `column_name=value`. Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file: (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises: ValueError: If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. TypeError: If any of the input arguments has a diffent type as documented \"\"\" self . _base_path = base_path if partitions is not None and rows_per_file != 0 : raise ValueError ( \"Only one of 'partitions' and 'rows_per_file' can be used.\" ) if rows_per_file != 0 : if not isinstance ( rows_per_file , int ): raise TypeError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if rows_per_file < 0 : raise ValueError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if partitions is not None : if isinstance ( partitions , ( str , bytes )): raise TypeError ( \"partitions must be an integer or an iterable of column names\" ) for _ in partitions : # Raises TypeError if not iterable break self . _partitions = partitions self . _rows_per_file = rows_per_file read_to_dict ( self , source , columns = None , row_filter = None , drop_duplicates = False , limit =- 1 , sample =- 1 ) \u00b6 Read a parquet dataset from disk into a dictionary of columns Source code in dframeio/parquet.py def read_to_dict ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> Dict [ str , List ]: \"\"\"Read a parquet dataset from disk into a dictionary of columns\"\"\" full_path = self . _validated_full_path ( source ) df = pq . read_table ( str ( full_path ), columns = columns , use_threads = True , use_pandas_metadata = True ) . to_pydict () if row_filter : # TODO: Pyarrow supports filtering on loading # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html raise NotImplementedError ( \"Row filtering is not implemented for dicts\" ) return df read_to_pandas ( self , source , columns = None , row_filter = None , drop_duplicates = False , limit =- 1 , sample =- 1 ) \u00b6 Read a parquet dataset from disk into a pandas dataframe Source code in dframeio/parquet.py def read_to_pandas ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> pd . DataFrame : \"\"\"Read a parquet dataset from disk into a pandas dataframe\"\"\" full_path = Path ( self . _base_path ) / source if Path ( self . _base_path ) not in full_path . parents : raise ValueError ( f \"The given source path { source } is not in base_path { self . _base_path } !\" ) # TODO: use read_pandas() # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_pandas.html#pyarrow.parquet.read_pandas df = pq . read_table ( str ( full_path ), columns = columns , use_threads = True , use_pandas_metadata = True ) . to_pandas () if row_filter : return df . query ( row_filter ) return df write_append ( self , target , dataframe ) \u00b6 Write data in append-mode Source code in dframeio/parquet.py def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data in append-mode\"\"\" # TODO: Implement raise NotImplementedError () write_replace ( self , target , dataframe ) \u00b6 Write data with full replacement of an existing dataset Parameters: Name Type Description Default target str The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the __init__ function. required dataframe Union[pandas.core.frame.DataFrame, Dict[str, List]] The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] required Exceptions: Type Description ValueError If the dataframe does not contain the columns to partition by as specified in the __init__ function. Source code in dframeio/parquet.py def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data with full replacement of an existing dataset Args: target: The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the [`__init__`](#dframeio.parquet.ParquetBackend.__init__) function. dataframe: The data to write as pandas.DataFrame or as a Python dictionary in the format `column_name: [column_data]` Raises: ValueError: If the dataframe does not contain the columns to partition by as specified in the [`__init__`](#dframeio.parquet.ParquetBackend.__init__) function. \"\"\" full_path = self . _validated_full_path ( target ) if full_path . exists (): if full_path . is_file (): full_path . unlink () elif full_path . is_dir (): shutil . rmtree ( str ( full_path ), ignore_errors = True ) if self . _rows_per_file > 0 : full_path . mkdir ( exist_ok = True ) for i in range ( 0 , self . _n_rows ( dataframe ), self . _rows_per_file ): pq . write_table ( self . _dataframe_slice_as_arrow_table ( dataframe , i , i + self . _rows_per_file ), where = str ( full_path / ( full_path . name + str ( i ))), flavor = \"spark\" , compression = \"snappy\" , ) else : arrow_table = self . _to_arrow_table ( dataframe ) if self . _partitions is not None : missing_columns = set ( self . _partitions ) - set ( arrow_table . column_names ) if missing_columns : raise ValueError ( f \"Expected the dataframe to have the partition columns { missing_columns } \" ) pq . write_to_dataset ( arrow_table , root_path = str ( full_path ), partition_cols = self . _partitions , flavor = \"spark\" , compression = \"snappy\" , ) else : pq . write_table ( arrow_table , where = str ( full_path ), flavor = \"spark\" , compression = \"snappy\" , )","title":"API documentation"},{"location":"api/#dframeio","text":"Top-level package for dataframe-io.","title":"dframeio"},{"location":"api/#dframeio.abstract","text":"Abstract interfaces for all storage backends","title":"abstract"},{"location":"api/#dframeio.abstract.AbstractDataFrameReader","text":"Interface for reading dataframes from different storage drivers","title":"AbstractDataFrameReader"},{"location":"api/#dframeio.abstract.AbstractDataFrameReader.read_to_dict","text":"Read data into a dict of named columns Source code in dframeio/abstract.py @abstractmethod def read_to_dict ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> Dict [ str , List ]: \"\"\"Read data into a dict of named columns\"\"\" raise NotImplementedError ()","title":"read_to_dict()"},{"location":"api/#dframeio.abstract.AbstractDataFrameReader.read_to_pandas","text":"Read data into a pandas.DataFrame Source code in dframeio/abstract.py @abstractmethod def read_to_pandas ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> pd . DataFrame : \"\"\"Read data into a pandas.DataFrame\"\"\" raise NotImplementedError ()","title":"read_to_pandas()"},{"location":"api/#dframeio.abstract.AbstractDataFrameWriter","text":"Interface for writing dataframes to different storage drivers","title":"AbstractDataFrameWriter"},{"location":"api/#dframeio.abstract.AbstractDataFrameWriter.write_append","text":"Write data in append-mode Source code in dframeio/abstract.py @abstractmethod def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data in append-mode\"\"\" raise NotImplementedError ()","title":"write_append()"},{"location":"api/#dframeio.abstract.AbstractDataFrameWriter.write_replace","text":"Write data with full replacement of an existing dataset Source code in dframeio/abstract.py @abstractmethod def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data with full replacement of an existing dataset\"\"\" raise NotImplementedError ()","title":"write_replace()"},{"location":"api/#dframeio.parquet","text":"Implementation to access parquet datasets using pyarrow.","title":"parquet"},{"location":"api/#dframeio.parquet.ParquetBackend","text":"Backend to read and write parquet datasets","title":"ParquetBackend"},{"location":"api/#dframeio.parquet.ParquetBackend.__init__","text":"Create a new ParquetBackend object Parameters: Name Type Description Default base_path str required partitions Iterable[str] (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: column_name=value . Per default data is written as a single file. Cannot be combined with rows_per_file. None rows_per_file int (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. 0 Exceptions: Type Description ValueError If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. TypeError If any of the input arguments has a diffent type as documented Source code in dframeio/parquet.py def __init__ ( self , base_path : str , partitions : Iterable [ str ] = None , rows_per_file : int = 0 ): \"\"\"Create a new ParquetBackend object Args: base_path: partitions: (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: `column_name=value`. Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file: (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises: ValueError: If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. TypeError: If any of the input arguments has a diffent type as documented \"\"\" self . _base_path = base_path if partitions is not None and rows_per_file != 0 : raise ValueError ( \"Only one of 'partitions' and 'rows_per_file' can be used.\" ) if rows_per_file != 0 : if not isinstance ( rows_per_file , int ): raise TypeError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if rows_per_file < 0 : raise ValueError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if partitions is not None : if isinstance ( partitions , ( str , bytes )): raise TypeError ( \"partitions must be an integer or an iterable of column names\" ) for _ in partitions : # Raises TypeError if not iterable break self . _partitions = partitions self . _rows_per_file = rows_per_file","title":"__init__()"},{"location":"api/#dframeio.parquet.ParquetBackend.read_to_dict","text":"Read a parquet dataset from disk into a dictionary of columns Source code in dframeio/parquet.py def read_to_dict ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> Dict [ str , List ]: \"\"\"Read a parquet dataset from disk into a dictionary of columns\"\"\" full_path = self . _validated_full_path ( source ) df = pq . read_table ( str ( full_path ), columns = columns , use_threads = True , use_pandas_metadata = True ) . to_pydict () if row_filter : # TODO: Pyarrow supports filtering on loading # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetDataset.html raise NotImplementedError ( \"Row filtering is not implemented for dicts\" ) return df","title":"read_to_dict()"},{"location":"api/#dframeio.parquet.ParquetBackend.read_to_pandas","text":"Read a parquet dataset from disk into a pandas dataframe Source code in dframeio/parquet.py def read_to_pandas ( self , source : str , columns : List [ str ] = None , row_filter : str = None , drop_duplicates : bool = False , limit : int = - 1 , sample : int = - 1 , ) -> pd . DataFrame : \"\"\"Read a parquet dataset from disk into a pandas dataframe\"\"\" full_path = Path ( self . _base_path ) / source if Path ( self . _base_path ) not in full_path . parents : raise ValueError ( f \"The given source path { source } is not in base_path { self . _base_path } !\" ) # TODO: use read_pandas() # https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_pandas.html#pyarrow.parquet.read_pandas df = pq . read_table ( str ( full_path ), columns = columns , use_threads = True , use_pandas_metadata = True ) . to_pandas () if row_filter : return df . query ( row_filter ) return df","title":"read_to_pandas()"},{"location":"api/#dframeio.parquet.ParquetBackend.write_append","text":"Write data in append-mode Source code in dframeio/parquet.py def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data in append-mode\"\"\" # TODO: Implement raise NotImplementedError ()","title":"write_append()"},{"location":"api/#dframeio.parquet.ParquetBackend.write_replace","text":"Write data with full replacement of an existing dataset Parameters: Name Type Description Default target str The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the __init__ function. required dataframe Union[pandas.core.frame.DataFrame, Dict[str, List]] The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] required Exceptions: Type Description ValueError If the dataframe does not contain the columns to partition by as specified in the __init__ function. Source code in dframeio/parquet.py def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data with full replacement of an existing dataset Args: target: The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the [`__init__`](#dframeio.parquet.ParquetBackend.__init__) function. dataframe: The data to write as pandas.DataFrame or as a Python dictionary in the format `column_name: [column_data]` Raises: ValueError: If the dataframe does not contain the columns to partition by as specified in the [`__init__`](#dframeio.parquet.ParquetBackend.__init__) function. \"\"\" full_path = self . _validated_full_path ( target ) if full_path . exists (): if full_path . is_file (): full_path . unlink () elif full_path . is_dir (): shutil . rmtree ( str ( full_path ), ignore_errors = True ) if self . _rows_per_file > 0 : full_path . mkdir ( exist_ok = True ) for i in range ( 0 , self . _n_rows ( dataframe ), self . _rows_per_file ): pq . write_table ( self . _dataframe_slice_as_arrow_table ( dataframe , i , i + self . _rows_per_file ), where = str ( full_path / ( full_path . name + str ( i ))), flavor = \"spark\" , compression = \"snappy\" , ) else : arrow_table = self . _to_arrow_table ( dataframe ) if self . _partitions is not None : missing_columns = set ( self . _partitions ) - set ( arrow_table . column_names ) if missing_columns : raise ValueError ( f \"Expected the dataframe to have the partition columns { missing_columns } \" ) pq . write_to_dataset ( arrow_table , root_path = str ( full_path ), partition_cols = self . _partitions , flavor = \"spark\" , compression = \"snappy\" , ) else : pq . write_table ( arrow_table , where = str ( full_path ), flavor = \"spark\" , compression = \"snappy\" , )","title":"write_replace()"},{"location":"authors/","text":"Credits \u00b6 Creator \u00b6 Christian Krudewig chr1st1ank@krudewig-online.de Contributors \u00b6 None yet. Why not be the first?","title":"Authors"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#creator","text":"Christian Krudewig chr1st1ank@krudewig-online.de","title":"Creator"},{"location":"authors/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/chr1st1ank/dframeio/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 dataframe-io could always use more documentation, whether as part of the official dataframe-io docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dframeio/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up dframeio for local development. Fork the dframeio repo on GitHub. Clone your fork locally 1 $ git clone git@github.com:your_name_here/dframeio.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 $ poetry install -E test -E doc -E dev Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 $ tox Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dframeio/actions and make sure that the tests pass for all supported Python versions. Tips``` \u00b6 1 $ pytest tests.test_dframeio ```To run a subset of tests. Deploying \u00b6 A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 3 $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Github actions will then deploy to PyPI if tests pass.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/chr1st1ank/dframeio/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"dataframe-io could always use more documentation, whether as part of the official dataframe-io docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dframeio/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up dframeio for local development. Fork the dframeio repo on GitHub. Clone your fork locally 1 $ git clone git@github.com:your_name_here/dframeio.git Ensure poetry is installed. Install dependencies and start your virtualenv: 1 $ poetry install -E test -E doc -E dev Create a branch for local development: 1 $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: 1 $ tox Commit your changes and push your branch to GitHub: 1 2 3 $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8 and 3.9. Check https://github.com/chr1st1ank/dframeio/actions and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#tips","text":"1 $ pytest tests.test_dframeio ```To run a subset of tests.","title":"Tips```"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. Make sure all your changes are committed (including an entry in HISTORY.md). Then run: 1 2 3 $ poetry patch # possible: major / minor / patch $ git push $ git push --tags Github actions will then deploy to PyPI if tests pass.","title":"Deploying"},{"location":"history/","text":"History \u00b6 0.1.0 (2021-05-23) \u00b6 First release on PyPI.","title":"History"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#010-2021-05-23","text":"First release on PyPI.","title":"0.1.0 (2021-05-23)"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install dataframe-io, run this command in your terminal: 1 $ pip install dframeio This is the preferred method to install dataframe-io, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for dataframe-io can be downloaded from the Github repo . You can either clone the public repository: 1 $ git clone git://github.com/chr1st1ank/dframeio Or download the tarball : 1 $ curl -OJL https://github.com/chr1st1ank/dframeio/tarball/master Once you have a copy of the source, you can install it with: 1 $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install dataframe-io, run this command in your terminal: 1 $ pip install dframeio This is the preferred method to install dataframe-io, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for dataframe-io can be downloaded from the Github repo . You can either clone the public repository: 1 $ git clone git://github.com/chr1st1ank/dframeio Or download the tarball : 1 $ curl -OJL https://github.com/chr1st1ank/dframeio/tarball/master Once you have a copy of the source, you can install it with: 1 $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use dataframe-io in a project 1 import dframeio","title":"Usage"},{"location":"usage/#usage","text":"To use dataframe-io in a project 1 import dframeio","title":"Usage"}]}