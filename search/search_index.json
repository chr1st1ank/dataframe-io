{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"dataframe-io \u00b6 Read and write dataframes from and to any storage. Documentation: https://chr1st1ank.github.io/dataframe-io/ License: Apache-2.0 Status: Initial development Features \u00b6 Dataframes types supported: pandas DataFrame Python dictionary Supported storage backends: Parquet files PostgreSQL database More backends will come. Open an issue if you are interested in a particular backend. Implementation status for reading data: Storage Select columns Filter rows Max rows Sampling Drop duplicates Parquet files \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714 \u00b9 PostgreSQL \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u00b9 only for pandas DataFrames Implementation status for writing data: Storage write append write replace Parquet files \u2714\ufe0f \u2714\ufe0f PostgreSQL \u2714\ufe0f \u2714\ufe0f Installation \u00b6 pip install dframeio # Including pyarrow to read/write parquet files: pip install dframeio[parquet] # Including PostgreSQL support: pip install dframeio[postgres] Show installed backends: >>> import dframeio >>> dframeio.backends [<class 'dframeio.parquet.ParquetBackend'>]","title":"Home"},{"location":"#dataframe-io","text":"Read and write dataframes from and to any storage. Documentation: https://chr1st1ank.github.io/dataframe-io/ License: Apache-2.0 Status: Initial development","title":"dataframe-io"},{"location":"#features","text":"Dataframes types supported: pandas DataFrame Python dictionary Supported storage backends: Parquet files PostgreSQL database More backends will come. Open an issue if you are interested in a particular backend. Implementation status for reading data: Storage Select columns Filter rows Max rows Sampling Drop duplicates Parquet files \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714 \u00b9 PostgreSQL \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u2714\ufe0f \u00b9 only for pandas DataFrames Implementation status for writing data: Storage write append write replace Parquet files \u2714\ufe0f \u2714\ufe0f PostgreSQL \u2714\ufe0f \u2714\ufe0f","title":"Features"},{"location":"#installation","text":"pip install dframeio # Including pyarrow to read/write parquet files: pip install dframeio[parquet] # Including PostgreSQL support: pip install dframeio[postgres] Show installed backends: >>> import dframeio >>> dframeio.backends [<class 'dframeio.parquet.ParquetBackend'>]","title":"Installation"},{"location":"authors/","text":"Credits \u00b6 Creator \u00b6 Christian Krudewig chr1st1ank@krudewig-online.de Contributors \u00b6 None yet. Why not be the first?","title":"Authors"},{"location":"authors/#credits","text":"","title":"Credits"},{"location":"authors/#creator","text":"Christian Krudewig chr1st1ank@krudewig-online.de","title":"Creator"},{"location":"authors/#contributors","text":"None yet. Why not be the first?","title":"Contributors"},{"location":"contributing/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/chr1st1ank/dframeio/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 dataframe-io could always use more documentation, whether as part of the official dataframe-io docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dframeio/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up dframeio for local development. Fork the dframeio repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dframeio.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8, 3.9 and 3.10. Make sure that the tests pass for all supported Python versions. Deploying \u00b6 A reminder for the maintainers on how to deploy. On branch \"main\": Adjust CHANGELOG.md as described on https://keepachangelog.com . Adjust the version number in dframeio/ init .py. Then run poetry version [major | minor | patch] . Commit and push the changes. Create a github release and watch the release workflow publishing the documentation and the PyPI package.","title":"Contributing"},{"location":"contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"contributing/#report-bugs","text":"Report bugs at https://github.com/chr1st1ank/dframeio/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"dataframe-io could always use more documentation, whether as part of the official dataframe-io docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/chr1st1ank/dframeio/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"contributing/#get-started","text":"Ready to contribute? Here's how to set up dframeio for local development. Fork the dframeio repo on GitHub. Clone your fork locally $ git clone git@github.com:your_name_here/dframeio.git Ensure poetry is installed. Install dependencies and start your virtualenv: $ poetry install -E test -E doc -E dev Create a branch for local development: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, check that your changes pass the tests, including testing other Python versions, with tox: $ tox Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring, and add the feature to the list in README.md. The pull request should work for CPython 3.7, 3.8, 3.9 and 3.10. Make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"contributing/#deploying","text":"A reminder for the maintainers on how to deploy. On branch \"main\": Adjust CHANGELOG.md as described on https://keepachangelog.com . Adjust the version number in dframeio/ init .py. Then run poetry version [major | minor | patch] . Commit and push the changes. Create a github release and watch the release workflow publishing the documentation and the PyPI package.","title":"Deploying"},{"location":"history/","text":"History \u00b6 Unreleased \u00b6 0.3.0 (2021-11-08) \u00b6 Added \u00b6 Support for PostgreSQL Less strict version pinning for dependencies Fixed \u00b6 Package is installable without pyarrow now 0.2.0 (2021-06-19) \u00b6 Added \u00b6 Parquet file reading and writing with all features drameio.filter module for row filters in SQL syntax 0.1.1 (2021-06-04) \u00b6 Working draft of parquet file reading and writing 0.1.0 (2021-05-23) \u00b6 First release on PyPI.","title":"History"},{"location":"history/#history","text":"","title":"History"},{"location":"history/#unreleased","text":"","title":"Unreleased"},{"location":"history/#030-2021-11-08","text":"","title":"0.3.0 (2021-11-08)"},{"location":"history/#added","text":"Support for PostgreSQL Less strict version pinning for dependencies","title":"Added"},{"location":"history/#fixed","text":"Package is installable without pyarrow now","title":"Fixed"},{"location":"history/#020-2021-06-19","text":"","title":"0.2.0 (2021-06-19)"},{"location":"history/#added_1","text":"Parquet file reading and writing with all features drameio.filter module for row filters in SQL syntax","title":"Added"},{"location":"history/#011-2021-06-04","text":"Working draft of parquet file reading and writing","title":"0.1.1 (2021-06-04)"},{"location":"history/#010-2021-05-23","text":"First release on PyPI.","title":"0.1.0 (2021-05-23)"},{"location":"installation/","text":"Installation \u00b6 Stable release \u00b6 To install dataframe-io, run this command in your terminal: $ pip install dframeio This is the preferred method to install dataframe-io, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process. From source \u00b6 The source for dataframe-io can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dataframe-io Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dataframe-io/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#stable-release","text":"To install dataframe-io, run this command in your terminal: $ pip install dframeio This is the preferred method to install dataframe-io, as it will always install the most recent stable release. If you don't have pip installed, this Python installation guide can guide you through the process.","title":"Stable release"},{"location":"installation/#from-source","text":"The source for dataframe-io can be downloaded from the Github repo . You can either clone the public repository: $ git clone git://github.com/chr1st1ank/dataframe-io Or download the tarball : $ curl -OJL https://github.com/chr1st1ank/dataframe-io/tarball/main Once you have a copy of the source, you can install it with: $ pip install .","title":"From source"},{"location":"usage/","text":"Usage \u00b6 To use dataframe-io in a project import dframeio","title":"Usage"},{"location":"usage/#usage","text":"To use dataframe-io in a project import dframeio","title":"Usage"},{"location":"api/dframeio.abstract/","text":"module dframeio. abstract </> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </> class dframeio.abstract. AbstractDataFrameReader ( ) </> Interface for reading dataframes from different storage drivers Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read data into a dict of named columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read data into a pandas.DataFrame </> abstract method read_to_pandas ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read data into a pandas.DataFrame Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. The filter and limit arguments are applied in the following order: first the row_filter expression is applied and all matching rows go into the next step, afterwards the limit argument is applied if given, in the next step the sample argument is applied if it is specified, at the very end drop_duplicates takes effect. This means that this flag may reduce the output size further and that fewer rows may be returned as specified with limit or sample if there are duplicates in the data. abstract method read_to_dict ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read data into a dict of named columns Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for read_to_pandas() . class dframeio.abstract. AbstractDataFrameWriter ( ) </> Interface for writing dataframes to different storage drivers Methods write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> abstract method write_replace ( target , dataframe ) </> Write data with full replacement of an existing dataset abstract method write_append ( target , dataframe ) </> Write data in append-mode","title":"dframeio.abstract"},{"location":"api/dframeio.abstract/#dframeioabstract","text":"</> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </> class","title":"dframeio.abstract"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframereader","text":"</> Interface for reading dataframes from different storage drivers Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read data into a dict of named columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read data into a pandas.DataFrame </> abstract method","title":"dframeio.abstract.AbstractDataFrameReader"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframereaderread_to_pandas","text":"</> Read data into a pandas.DataFrame Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. The filter and limit arguments are applied in the following order: first the row_filter expression is applied and all matching rows go into the next step, afterwards the limit argument is applied if given, in the next step the sample argument is applied if it is specified, at the very end drop_duplicates takes effect. This means that this flag may reduce the output size further and that fewer rows may be returned as specified with limit or sample if there are duplicates in the data. abstract method","title":"dframeio.abstract.AbstractDataFrameReader.read_to_pandas"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframereaderread_to_dict","text":"</> Read data into a dict of named columns Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for read_to_pandas() . class","title":"dframeio.abstract.AbstractDataFrameReader.read_to_dict"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframewriter","text":"</> Interface for writing dataframes to different storage drivers Methods write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> abstract method","title":"dframeio.abstract.AbstractDataFrameWriter"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframewriterwrite_replace","text":"</> Write data with full replacement of an existing dataset abstract method","title":"dframeio.abstract.AbstractDataFrameWriter.write_replace"},{"location":"api/dframeio.abstract/#dframeioabstractabstractdataframewriterwrite_append","text":"</> Write data in append-mode","title":"dframeio.abstract.AbstractDataFrameWriter.write_append"},{"location":"api/dframeio.filter/","text":"module dframeio. filter </> Filter expressions for data reading operations with predicate pushdown. This module is responsible for translate filter expressions from a simplified SQL syntax into different formats understood by the various backends. This way the same language can be used to implement filtering regardless of the data source. The grammar of the filter statements is the same as in a WHERE clause in SQL. Supported features: Comparing column values to numbers, strings and another column's values using the operators > < = != >= <= , e.g. a.column < 5 Comparison against a set of values with \u00ccN and NOT IN , e.g. a.column IN (1, 2, 3) Boolean combination of conditions with AND , OR and \u01f8OT NULL comparison as in a IS NULL or b IS NOT NULL Strings can be quoted with single-quotes and double-quotes. Column names can but don't have to be quoted with SQL quotes (backticks). E.g.: `a.column` = \"abc\" AND b IS NOT NULL OR index < 50 Functions to_prefix_notation ( statement ) (str) \u2014 Parse a filter statement and return it in prefix notation. </> to_psql ( statement ) (str) \u2014 Convert a filter statement to Postgres SQL syntax </> to_pyarrow_dnf ( statement ) (Union(list of list of (str, str, any), list of (str, str, any), (str, str, any))) \u2014 Convert a filter statement to the disjunctive normal form understood by pyarrow </> function dframeio.filter. to_prefix_notation ( statement ) </> Parse a filter statement and return it in prefix notation. Parameters statement (str) \u2014 A filter predicate as string Returns (str) The filter statement in prefix notation (polish notation) as string Examples >>> to_prefix_notation(\"a.column != 0\") '(!= Column<a.column> 0)' >>> to_prefix_notation(\"a > 1 and b <= 3\") '(AND (> Column<a> 1) (<= Column<b> 3))' function dframeio.filter. to_pyarrow_dnf ( statement ) </> Convert a filter statement to the disjunctive normal form understood by pyarrow Predicates are expressed in disjunctive normal form (DNF), like [[('x', '=', 0), ...], ...] . The outer list is understood as chain of disjunctions (\"or\"), every inner list as a chain of conjunctions (\"and\"). The inner lists contain tuples with a single operation in infix notation each. More information about the format and its limitations can be found in the pyarrow documentation . Parameters statement (str) \u2014 A filter predicate as string Returns (Union(list of list of (str, str, any), list of (str, str, any), (str, str, any))) The filter statement converted to a list of lists of tuples. Raises ValueError \u2014 If the statement cannot be parsed Examples >>> to_pyarrow_dnf(\"a.column != 0\") [[('a.column', '!=', 0)]] >>> to_pyarrow_dnf(\"a > 1 and b <= 3\") [[('a', '>', 1), ('b', '<=', 3)]] >>> to_pyarrow_dnf(\"a > 1 and b <= 3 or c = 'abc'\") [[('a', '>', 1), ('b', '<=', 3)], [('c', '=', 'abc')]] function dframeio.filter. to_psql ( statement ) </> Convert a filter statement to Postgres SQL syntax Parameters statement (str) \u2014 A filter predicate as string Returns (str) The filter statement converted to psql. Raises ValueError \u2014 If the statement cannot be parsed Examples >>> to_psql(\"a.column != 0\") '\"a.column\" <> 0' >>> to_psql(\"a > 1 and b <= 3\") '\"a\" > 1 AND \"b\" <= 3' >>> to_psql(\"a > 1 and b <= 3 or c = 'abc'\") '\"a\" > 1 AND \"b\" <= 3 OR \"c\" = \\'abc\\''","title":"dframeio.filter"},{"location":"api/dframeio.filter/#dframeiofilter","text":"</> Filter expressions for data reading operations with predicate pushdown. This module is responsible for translate filter expressions from a simplified SQL syntax into different formats understood by the various backends. This way the same language can be used to implement filtering regardless of the data source. The grammar of the filter statements is the same as in a WHERE clause in SQL. Supported features: Comparing column values to numbers, strings and another column's values using the operators > < = != >= <= , e.g. a.column < 5 Comparison against a set of values with \u00ccN and NOT IN , e.g. a.column IN (1, 2, 3) Boolean combination of conditions with AND , OR and \u01f8OT NULL comparison as in a IS NULL or b IS NOT NULL Strings can be quoted with single-quotes and double-quotes. Column names can but don't have to be quoted with SQL quotes (backticks). E.g.: `a.column` = \"abc\" AND b IS NOT NULL OR index < 50 Functions to_prefix_notation ( statement ) (str) \u2014 Parse a filter statement and return it in prefix notation. </> to_psql ( statement ) (str) \u2014 Convert a filter statement to Postgres SQL syntax </> to_pyarrow_dnf ( statement ) (Union(list of list of (str, str, any), list of (str, str, any), (str, str, any))) \u2014 Convert a filter statement to the disjunctive normal form understood by pyarrow </> function","title":"dframeio.filter"},{"location":"api/dframeio.filter/#dframeiofilterto_prefix_notation","text":"</> Parse a filter statement and return it in prefix notation. Parameters statement (str) \u2014 A filter predicate as string Returns (str) The filter statement in prefix notation (polish notation) as string Examples >>> to_prefix_notation(\"a.column != 0\") '(!= Column<a.column> 0)' >>> to_prefix_notation(\"a > 1 and b <= 3\") '(AND (> Column<a> 1) (<= Column<b> 3))' function","title":"dframeio.filter.to_prefix_notation"},{"location":"api/dframeio.filter/#dframeiofilterto_pyarrow_dnf","text":"</> Convert a filter statement to the disjunctive normal form understood by pyarrow Predicates are expressed in disjunctive normal form (DNF), like [[('x', '=', 0), ...], ...] . The outer list is understood as chain of disjunctions (\"or\"), every inner list as a chain of conjunctions (\"and\"). The inner lists contain tuples with a single operation in infix notation each. More information about the format and its limitations can be found in the pyarrow documentation . Parameters statement (str) \u2014 A filter predicate as string Returns (Union(list of list of (str, str, any), list of (str, str, any), (str, str, any))) The filter statement converted to a list of lists of tuples. Raises ValueError \u2014 If the statement cannot be parsed Examples >>> to_pyarrow_dnf(\"a.column != 0\") [[('a.column', '!=', 0)]] >>> to_pyarrow_dnf(\"a > 1 and b <= 3\") [[('a', '>', 1), ('b', '<=', 3)]] >>> to_pyarrow_dnf(\"a > 1 and b <= 3 or c = 'abc'\") [[('a', '>', 1), ('b', '<=', 3)], [('c', '=', 'abc')]] function","title":"dframeio.filter.to_pyarrow_dnf"},{"location":"api/dframeio.filter/#dframeiofilterto_psql","text":"</> Convert a filter statement to Postgres SQL syntax Parameters statement (str) \u2014 A filter predicate as string Returns (str) The filter statement converted to psql. Raises ValueError \u2014 If the statement cannot be parsed Examples >>> to_psql(\"a.column != 0\") '\"a.column\" <> 0' >>> to_psql(\"a > 1 and b <= 3\") '\"a\" > 1 AND \"b\" <= 3' >>> to_psql(\"a > 1 and b <= 3 or c = 'abc'\") '\"a\" > 1 AND \"b\" <= 3 OR \"c\" = \\'abc\\''","title":"dframeio.filter.to_psql"},{"location":"api/dframeio/","text":"package dframeio </> Top-level package for dataframe-io. module dframeio. filter </> Filter expressions for data reading operations with predicate pushdown. This module is responsible for translate filter expressions from a simplified SQL syntax into different formats understood by the various backends. This way the same language can be used to implement filtering regardless of the data source. The grammar of the filter statements is the same as in a WHERE clause in SQL. Supported features: Comparing column values to numbers, strings and another column's values using the operators > < = != >= <= , e.g. a.column < 5 Comparison against a set of values with \u00ccN and NOT IN , e.g. a.column IN (1, 2, 3) Boolean combination of conditions with AND , OR and \u01f8OT NULL comparison as in a IS NULL or b IS NOT NULL Strings can be quoted with single-quotes and double-quotes. Column names can but don't have to be quoted with SQL quotes (backticks). E.g.: `a.column` = \"abc\" AND b IS NOT NULL OR index < 50 Functions to_prefix_notation ( statement ) (str) \u2014 Parse a filter statement and return it in prefix notation. </> to_psql ( statement ) (str) \u2014 Convert a filter statement to Postgres SQL syntax </> to_pyarrow_dnf ( statement ) (Union(list of list of (str, str, any), list of (str, str, any), (str, str, any))) \u2014 Convert a filter statement to the disjunctive normal form understood by pyarrow </> module dframeio. abstract </> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </> module dframeio. parquet </> Access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> module dframeio. postgres </> Access PostgreSQL databases using psycopg3. Classes PostgresBackend \u2014 Backend to read and write PostgreSQL tables </>","title":"dframeio"},{"location":"api/dframeio/#dframeio","text":"</> Top-level package for dataframe-io. module","title":"dframeio"},{"location":"api/dframeio/#dframeiofilter","text":"</> Filter expressions for data reading operations with predicate pushdown. This module is responsible for translate filter expressions from a simplified SQL syntax into different formats understood by the various backends. This way the same language can be used to implement filtering regardless of the data source. The grammar of the filter statements is the same as in a WHERE clause in SQL. Supported features: Comparing column values to numbers, strings and another column's values using the operators > < = != >= <= , e.g. a.column < 5 Comparison against a set of values with \u00ccN and NOT IN , e.g. a.column IN (1, 2, 3) Boolean combination of conditions with AND , OR and \u01f8OT NULL comparison as in a IS NULL or b IS NOT NULL Strings can be quoted with single-quotes and double-quotes. Column names can but don't have to be quoted with SQL quotes (backticks). E.g.: `a.column` = \"abc\" AND b IS NOT NULL OR index < 50 Functions to_prefix_notation ( statement ) (str) \u2014 Parse a filter statement and return it in prefix notation. </> to_psql ( statement ) (str) \u2014 Convert a filter statement to Postgres SQL syntax </> to_pyarrow_dnf ( statement ) (Union(list of list of (str, str, any), list of (str, str, any), (str, str, any))) \u2014 Convert a filter statement to the disjunctive normal form understood by pyarrow </> module","title":"dframeio.filter"},{"location":"api/dframeio/#dframeioabstract","text":"</> Abstract interfaces for all storage backends Classes AbstractDataFrameReader \u2014 Interface for reading dataframes from different storage drivers </> AbstractDataFrameWriter \u2014 Interface for writing dataframes to different storage drivers </> module","title":"dframeio.abstract"},{"location":"api/dframeio/#dframeioparquet","text":"</> Access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> module","title":"dframeio.parquet"},{"location":"api/dframeio/#dframeiopostgres","text":"</> Access PostgreSQL databases using psycopg3. Classes PostgresBackend \u2014 Backend to read and write PostgreSQL tables </>","title":"dframeio.postgres"},{"location":"api/dframeio.parquet/","text":"module dframeio. parquet </> Access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> class dframeio.parquet. ParquetBackend ( base_path , partitions=None , rows_per_file=0 ) </> Bases dframeio.abstract.AbstractDataFrameReader dframeio.abstract.AbstractDataFrameWriter Backend to read and write parquet datasets Parameters base_path (str) \u2014 Base path for the parquet files. Only files in this folder or subfolders can be read from or written to. partitions (iterable of str, optional) \u2014 (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: column_name=value . Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file (int, optional) \u2014 (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises TypeError \u2014 If any of the input arguments has a diffent type as documented ValueError \u2014 If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read a parquet dataset from disk into a dictionary of columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read a parquet dataset from disk into a pandas DataFrame </> write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> method read_to_pandas ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read a parquet dataset from disk into a pandas DataFrame Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows limit (int, optional) \u2014 Maximum number of rows to return (limit to first n rows) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. Raises ValueError \u2014 If path specified with source is outside of the base path The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method read_to_dict ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read a parquet dataset from disk into a dictionary of columns Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows limit (int, optional) \u2014 Maximum number of rows to return (limit to first n rows) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 (Not supported!) Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values Raises NotImplementedError \u2014 When drop_duplicates is specified The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method write_replace ( target , dataframe ) </> Write data with full replacement of an existing dataset Parameters target (str) \u2014 The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the __init__() function. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises TypeError \u2014 When the dataframe is neither an pandas.DataFrame nor a dictionary ValueError \u2014 If the dataframe does not contain the columns to partition by as specified in the __init__() function. method write_append ( target , dataframe ) </> Write data in append-mode","title":"dframeio.parquet"},{"location":"api/dframeio.parquet/#dframeioparquet","text":"</> Access parquet datasets using pyarrow. Classes ParquetBackend \u2014 Backend to read and write parquet datasets </> class","title":"dframeio.parquet"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackend","text":"</> Bases dframeio.abstract.AbstractDataFrameReader dframeio.abstract.AbstractDataFrameWriter Backend to read and write parquet datasets Parameters base_path (str) \u2014 Base path for the parquet files. Only files in this folder or subfolders can be read from or written to. partitions (iterable of str, optional) \u2014 (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: column_name=value . Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file (int, optional) \u2014 (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises TypeError \u2014 If any of the input arguments has a diffent type as documented ValueError \u2014 If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read a parquet dataset from disk into a dictionary of columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read a parquet dataset from disk into a pandas DataFrame </> write_append ( target , dataframe ) \u2014 Write data in append-mode </> write_replace ( target , dataframe ) \u2014 Write data with full replacement of an existing dataset </> method","title":"dframeio.parquet.ParquetBackend"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendread_to_pandas","text":"</> Read a parquet dataset from disk into a pandas DataFrame Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows limit (int, optional) \u2014 Maximum number of rows to return (limit to first n rows) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. Raises ValueError \u2014 If path specified with source is outside of the base path The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method","title":"dframeio.parquet.ParquetBackend.read_to_pandas"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendread_to_dict","text":"</> Read a parquet dataset from disk into a dictionary of columns Parameters source (str) \u2014 The path of the file or folder with a parquet dataset to read columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows limit (int, optional) \u2014 Maximum number of rows to return (limit to first n rows) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 (Not supported!) Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values Raises NotImplementedError \u2014 When drop_duplicates is specified The logic of the filtering arguments is as documented for AbstractDataFrameReader.read_to_pandas() . method","title":"dframeio.parquet.ParquetBackend.read_to_dict"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendwrite_replace","text":"</> Write data with full replacement of an existing dataset Parameters target (str) \u2014 The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the __init__() function. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises TypeError \u2014 When the dataframe is neither an pandas.DataFrame nor a dictionary ValueError \u2014 If the dataframe does not contain the columns to partition by as specified in the __init__() function. method","title":"dframeio.parquet.ParquetBackend.write_replace"},{"location":"api/dframeio.parquet/#dframeioparquetparquetbackendwrite_append","text":"</> Write data in append-mode","title":"dframeio.parquet.ParquetBackend.write_append"},{"location":"api/dframeio.postgres/","text":"module dframeio. postgres </> Access PostgreSQL databases using psycopg3. Classes PostgresBackend \u2014 Backend to read and write PostgreSQL tables </> class dframeio.postgres. PostgresBackend ( conninfo=None , autocommit=True ) </> Bases dframeio.abstract.AbstractDataFrameReader dframeio.abstract.AbstractDataFrameWriter Backend to read and write PostgreSQL tables Parameters conninfo (str, optional) \u2014 Connection string in libq format. See the PostgreSQL docs for details. Raises TypeError \u2014 If any of the input arguments has a diffent type as documented ValueError \u2014 If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read data into a dict of named columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read a postgres table into a pandas DataFrame </> write_append ( target , dataframe ) \u2014 Write data in append-mode to a Postgres table </> write_replace ( target , dataframe ) \u2014 Write data to a Postgres table after deleting all the existing content </> method read_to_pandas ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read a postgres table into a pandas DataFrame Parameters source (str) \u2014 The table name (may include a database name) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows limit (int, optional) \u2014 Maximum number of rows to return (limit to first n rows) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. Raises ValueError \u2014 If path specified with source is outside of the base path The logic of the filtering arguments is as documented for read_to_pandas() . method read_to_dict ( source , columns=None , row_filter=None , limit=-1 , sample=-1 , drop_duplicates=False ) </> Read data into a dict of named columns Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for read_to_pandas() . method write_replace ( target , dataframe ) </> Write data to a Postgres table after deleting all the existing content Parameters target (str) \u2014 The database table to write to. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises TypeError \u2014 When the dataframe is neither a pandas.DataFrame nor a dictionary method write_append ( target , dataframe ) </> Write data in append-mode to a Postgres table Parameters target (str) \u2014 The database table to write to. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises TypeError \u2014 When the dataframe is neither a pandas.DataFrame nor a dictionary","title":"dframeio.postgres"},{"location":"api/dframeio.postgres/#dframeiopostgres","text":"</> Access PostgreSQL databases using psycopg3. Classes PostgresBackend \u2014 Backend to read and write PostgreSQL tables </> class","title":"dframeio.postgres"},{"location":"api/dframeio.postgres/#dframeiopostgrespostgresbackend","text":"</> Bases dframeio.abstract.AbstractDataFrameReader dframeio.abstract.AbstractDataFrameWriter Backend to read and write PostgreSQL tables Parameters conninfo (str, optional) \u2014 Connection string in libq format. See the PostgreSQL docs for details. Raises TypeError \u2014 If any of the input arguments has a diffent type as documented ValueError \u2014 If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. Methods read_to_dict ( source , columns , row_filter , limit , sample , drop_duplicates ) (dict(str: )) \u2014 Read data into a dict of named columns </> read_to_pandas ( source , columns , row_filter , limit , sample , drop_duplicates ) (DataFrame) \u2014 Read a postgres table into a pandas DataFrame </> write_append ( target , dataframe ) \u2014 Write data in append-mode to a Postgres table </> write_replace ( target , dataframe ) \u2014 Write data to a Postgres table after deleting all the existing content </> method","title":"dframeio.postgres.PostgresBackend"},{"location":"api/dframeio.postgres/#dframeiopostgrespostgresbackendread_to_pandas","text":"</> Read a postgres table into a pandas DataFrame Parameters source (str) \u2014 The table name (may include a database name) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 Filter expression for selecting rows limit (int, optional) \u2014 Maximum number of rows to return (limit to first n rows) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows from the final selection Returns (DataFrame) A pandas DataFrame with the requested data. Raises ValueError \u2014 If path specified with source is outside of the base path The logic of the filtering arguments is as documented for read_to_pandas() . method","title":"dframeio.postgres.PostgresBackend.read_to_pandas"},{"location":"api/dframeio.postgres/#dframeiopostgrespostgresbackendread_to_dict","text":"</> Read data into a dict of named columns Parameters source (str) \u2014 A string specifying the data source (format differs by backend) columns (list of str, optional) \u2014 List of column names to limit the reading to row_filter (str, optional) \u2014 NOT IMPLEMENTED. Reserved keyword for filtering rows. limit (int, optional) \u2014 Maximum number of rows to return (top-n) sample (int, optional) \u2014 Size of a random sample to return drop_duplicates (bool, optional) \u2014 Whether to drop duplicate rows Returns (dict(str: )) A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for read_to_pandas() . method","title":"dframeio.postgres.PostgresBackend.read_to_dict"},{"location":"api/dframeio.postgres/#dframeiopostgrespostgresbackendwrite_replace","text":"</> Write data to a Postgres table after deleting all the existing content Parameters target (str) \u2014 The database table to write to. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises TypeError \u2014 When the dataframe is neither a pandas.DataFrame nor a dictionary method","title":"dframeio.postgres.PostgresBackend.write_replace"},{"location":"api/dframeio.postgres/#dframeiopostgrespostgresbackendwrite_append","text":"</> Write data in append-mode to a Postgres table Parameters target (str) \u2014 The database table to write to. dataframe (Union(dataframe, dict(str: ))) \u2014 The data to write as pandas.DataFrame or as a Python dictionary in the format column_name: [column_data] Raises TypeError \u2014 When the dataframe is neither a pandas.DataFrame nor a dictionary","title":"dframeio.postgres.PostgresBackend.write_append"},{"location":"api/source/dframeio.abstract/","text":"SOURCE CODE dframeio. abstract DOCS \"\"\"Abstract interfaces for all storage backends\"\"\" from abc import abstractmethod from typing import Dict , List , Union try : import pandas as pd except ImportError : pd = None class AbstractDataFrameReader : DOCS \"\"\"Interface for reading dataframes from different storage drivers\"\"\" @abstractmethod DOCS def read_to_pandas ( self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> pd . DataFrame : \"\"\"Read data into a pandas.DataFrame Args: source: A string specifying the data source (format differs by backend) columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows. limit: Maximum number of rows to return (top-n) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows from the final selection Returns: A pandas DataFrame with the requested data. The filter and limit arguments are applied in the following order: - first the `row_filter` expression is applied and all matching rows go into the next step, - afterwards the `limit` argument is applied if given, - in the next step the `sample` argument is applied if it is specified, - at the very end `drop_duplicates` takes effect. This means that this flag may reduce the output size further and that fewer rows may be returned as specified with `limit` or `sample` if there are duplicates in the data. \"\"\" raise NotImplementedError () @abstractmethod DOCS def read_to_dict ( self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> Dict [ str , List ]: \"\"\"Read data into a dict of named columns Args: source: A string specifying the data source (format differs by backend) columns: List of column names to limit the reading to row_filter: NOT IMPLEMENTED. Reserved keyword for filtering rows. limit: Maximum number of rows to return (top-n) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows Returns: A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for [`read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" raise NotImplementedError () class AbstractDataFrameWriter : DOCS \"\"\"Interface for writing dataframes to different storage drivers\"\"\" @abstractmethod DOCS def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data with full replacement of an existing dataset\"\"\" raise NotImplementedError () @abstractmethod DOCS def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Write data in append-mode\"\"\" raise NotImplementedError ()","title":"dframeio.abstract"},{"location":"api/source/dframeio.filter/","text":"SOURCE CODE dframeio. filter DOCS \"\"\"Filter expressions for data reading operations with predicate pushdown. This module is responsible for translate filter expressions from a simplified SQL syntax into different formats understood by the various backends. This way the same language can be used to implement filtering regardless of the data source. The grammar of the filter statements is the same as in a WHERE clause in SQL. Supported features: - Comparing column values to numbers, strings and another column's values using the operators `> < = != >= <=`, e.g. `a.column < 5` - Comparison against a set of values with \u00ccN and `NOT IN`, e.g. `a.column IN (1, 2, 3)` - Boolean combination of conditions with `AND`, `OR` and `\u01f8OT` - `NULL` comparison as in `a IS NULL` or `b IS NOT NULL` Strings can be quoted with single-quotes and double-quotes. Column names can but don't have to be quoted with SQL quotes (backticks). E.g.: ```sql `a.column` = \"abc\" AND b IS NOT NULL OR index < 50 ``` \"\"\" from dataclasses import dataclass from typing import Any , List , Tuple , Union import lark # See SQL 2003 working draft http://www.wiscorp.com/sql20nn.zip (Part 2, section 6.35) lark_grammar = r \"\"\" ?boolean_value_exp: boolean_term | boolean_value_exp \"OR\"i boolean_term -> or_operation ?boolean_term: boolean_factor | boolean_term \"AND\"i boolean_factor -> and_operation ?boolean_factor: \"(\" boolean_value_exp \")\" | single_condition | \"NOT\" single_condition -> negation ?single_condition: comparison | null_comparison | notin_list | in_list comparison: columnname BINOP column_rval null_comparison: columnname (ISNULL | NOTNULL) notin_list: columnname NOTIN literal_list in_list: columnname \"IN\"i literal_list literal_list: (\"(\") literal (\",\" literal)* (\")\") ?literal: SIGNED_NUMBER | ESCAPED_STRING ?column_rval: columnname | SIGNED_NUMBER | ESCAPED_STRING ?columnname: \"`\" columnname \"`\" | UNQUOTED_COLUMNNAME ISNULL.9: /IS\\s+NULL/i NOTNULL.10: /IS\\s+NOT\\s+NULL/i NOTIN.10: /NOT\\s+IN/i NOT.9: \"NOT\"i UNQUOTED_COLUMNNAME.1: NAMECHAR+ (\".\" + NAMECHAR+)* NAMECHAR: \"_\"|\"$\"|LETTER|DIGIT BINOP.10: \"!=\" | \">=\" | \"<=\" | \"<\" | \">\" | \"=\" ESCAPED_STRING.2 : DOUBLE_QUOTE_ESCAPED_STRING | SINGLE_QUOTE_ESCAPED_STRING DOUBLE_QUOTE_ESCAPED_STRING.2 : \"\\\"\" _STRING_ESC_INNER \"\\\"\" SINGLE_QUOTE_ESCAPED_STRING.2 : \"'\" _STRING_ESC_INNER \"'\" %i mport common.LETTER %i mport common.DIGIT %i mport common.SIGNED_NUMBER %i mport common._STRING_ESC_INNER %i mport common.WS_INLINE %i gnore WS_INLINE \"\"\" def _make_parser ( transformer : lark . Transformer ): return lark . Lark ( lark_grammar , start = \"boolean_value_exp\" , lexer = \"standard\" , parser = \"lalr\" , transformer = transformer , ) class _PrefixNotationTransformer ( lark . Transformer ): \"\"\"lark.Transformer to translate to polish/prefix notation\"\"\" # pylint: disable=missing-function-docstring,no-self-use def and_operation ( self , operands : lark . Token ): return self . format_operation ( \"AND\" , * operands ) def or_operation ( self , operands : lark . Token ): return self . format_operation ( \"OR\" , * operands ) @lark . v_args ( inline = True ) def comparison ( self , left : lark . Token , operator : lark . Token , right : lark . Token ): return self . format_operation ( operator , left , right ) @lark . v_args ( inline = True ) def null_comparison ( self , operand : lark . Token , operator : lark . Token ): if operator . type == \"ISNULL\" : return self . format_operation ( \"ISNULL\" , operand ) if operator . type == \"NOTNULL\" : return self . format_operation ( \"NOTNULL\" , operand ) raise lark . ParseError ( \"Invalid NULL comparison\" ) @lark . v_args ( inline = True ) def negation ( self , operand : lark . Token ): return self . format_operation ( \"NOT\" , operand ) @lark . v_args ( inline = True ) def notin_list ( self , column : lark . Token , _ : lark . Token , lst : lark . Token ): return self . format_operation ( \"NOTIN\" , column , lst ) @lark . v_args ( inline = True ) def in_list ( self , column : lark . Token , lst : lark . Token ): return self . format_operation ( \"IN\" , column , lst ) def literal_list ( self , members : lark . Token ): return \"[\" + \",\" . join ( m for m in members ) + \"]\" def UNQUOTED_COLUMNNAME ( self , name : lark . Token ): return f \"Column< { name . value } >\" def ESCAPED_STRING ( self , tok : lark . Token ): assert len ( tok ) > 1 and ( tok [ 0 ] == tok [ - 1 ] == \"'\" or tok [ 0 ] == tok [ - 1 ] == '\"' ) return \"'\" + tok [ 1 : - 1 ] + \"'\" def SIGNED_NUMBER ( self , tok : lark . Token ): \"\"\"Convert the value of `tok` from string to number\"\"\" try : return str ( int ( tok )) except ValueError : return str ( float ( tok )) @staticmethod def format_operation ( operator , * operands ): return f \"( { operator } { ' ' . join ( operands ) } )\" def to_prefix_notation ( statement : str ) -> str : DOCS \"\"\"Parse a filter statement and return it in prefix notation. Args: statement: A filter predicate as string Returns: The filter statement in prefix notation (polish notation) as string Examples: >>> to_prefix_notation(\"a.column != 0\") '(!= Column<a.column> 0)' >>> to_prefix_notation(\"a > 1 and b <= 3\") '(AND (> Column<a> 1) (<= Column<b> 3))' \"\"\" parser = _make_parser ( _PrefixNotationTransformer ()) return parser . parse ( statement ) class _PyarrowDNFTransformer ( lark . Transformer ): \"\"\"lark.Transformer to translate to pyarrow's special DNF format\"\"\" # pylint: disable=missing-function-docstring,missing-class-docstring,no-self-use @dataclass class Column : name : str @dataclass class Or : operands : list def format ( self ) -> List [ List [ Tuple [ str , str , Any ]]]: disjunction = [] for o in self . operands : if isinstance ( o , _PyarrowDNFTransformer . And ): disjunction . append ( o . format ()) elif isinstance ( o , _PyarrowDNFTransformer . Condition ): disjunction . append ([ o . format ()]) else : raise ValueError ( \"For PyArrow only disjunctions of conjunctions \" \"or simple conditions are allowed. \" f \"` { o } ` is not a single condition nor a conjunction.\" ) return disjunction @dataclass class And : operands : list def format ( self ) -> List [ Tuple [ str , str , Any ]]: conjunction = [] for o in self . operands : if isinstance ( o , _PyarrowDNFTransformer . Condition ): conjunction . append ( o . format ()) else : raise ValueError ( \"For PyArrow only conjunctions of simple conditions are allowed.\" ) return conjunction @dataclass class Condition : key : str operator : str value : Union [ str , int , float ] def format ( self ) -> Tuple [ str , str , Any ]: if not isinstance ( self . key , _PyarrowDNFTransformer . Column ): raise ValueError ( \"For pyarrow only comparisons of the form `key <operator> value`\" \"are allowed, where `key` must be a column name.\" ) if not isinstance ( self . value , ( int , float , str , set )): raise ValueError ( \"For pyarrow only comparisons of the form `key <operator> value` are allowed.\" ) return self . key . name , self . operator , self . value def and_operation ( self , operands : lark . Token ): return _PyarrowDNFTransformer . And ( list ( operands )) def or_operation ( self , operands : lark . Token ): return _PyarrowDNFTransformer . Or ( list ( operands )) @lark . v_args ( inline = True ) def comparison ( self , left : lark . Token , operator : lark . Token , right : lark . Token ): return _PyarrowDNFTransformer . Condition ( left , operator . value , right ) @lark . v_args ( inline = True ) def null_comparison ( self , operand : lark . Token , operator : lark . Token ): if operator . type == \"ISNULL\" : return _PyarrowDNFTransformer . Condition ( operand , \"=\" , \"null\" ) if operator . type == \"NOTNULL\" : return _PyarrowDNFTransformer . Condition ( operand , \"!=\" , \"null\" ) raise lark . ParseError ( \"Invalid NULL comparison\" ) def negation ( self , _ ): raise ValueError ( \"Pyarrow doesn't support the `NOT` operator\" ) @lark . v_args ( inline = True ) def notin_list ( self , column : lark . Token , _ : lark . Token , lst : lark . Token ): return _PyarrowDNFTransformer . Condition ( column , \"not in\" , lst ) @lark . v_args ( inline = True ) def in_list ( self , column : lark . Token , lst : lark . Token ): return _PyarrowDNFTransformer . Condition ( column , \"in\" , lst ) def literal_list ( self , members : lark . Token ): return set ( members ) def UNQUOTED_COLUMNNAME ( self , name : lark . Token ): return _PyarrowDNFTransformer . Column ( name . value ) def ESCAPED_STRING ( self , tok : lark . Token ): assert len ( tok ) > 1 and ( tok [ 0 ] == tok [ - 1 ] == \"'\" or tok [ 0 ] == tok [ - 1 ] == '\"' ) return tok [ 1 : - 1 ] def SIGNED_NUMBER ( self , tok : lark . Token ): \"\"\"Convert the value of `tok` from string to number\"\"\" try : return int ( tok ) except ValueError : return float ( tok ) def _raise_error ( e : Exception ): raise e PyArrowDNFType = Union [ List [ List [ Tuple [ str , str , Any ]]], List [ Tuple [ str , str , Any ]], Tuple [ str , str , Any ] ] def to_pyarrow_dnf ( statement : str ) -> PyArrowDNFType : DOCS \"\"\"Convert a filter statement to the disjunctive normal form understood by pyarrow Predicates are expressed in disjunctive normal form (DNF), like `[[('x', '=', 0), ...], ...]`. The outer list is understood as chain of disjunctions (\"or\"), every inner list as a chain of conjunctions (\"and\"). The inner lists contain tuples with a single operation in infix notation each. More information about the format and its limitations can be found in the [pyarrow documentation](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.read_table.html#pyarrow-parquet-read-table). Args: statement: A filter predicate as string Returns: The filter statement converted to a list of lists of tuples. Raises: ValueError: If the statement cannot be parsed Examples: >>> to_pyarrow_dnf(\"a.column != 0\") [[('a.column', '!=', 0)]] >>> to_pyarrow_dnf(\"a > 1 and b <= 3\") [[('a', '>', 1), ('b', '<=', 3)]] >>> to_pyarrow_dnf(\"a > 1 and b <= 3 or c = 'abc'\") [[('a', '>', 1), ('b', '<=', 3)], [('c', '=', 'abc')]] \"\"\" # noqa: E501 - Flake8 line-to-long: The link above cannot be shortened. parser = _make_parser ( _PyarrowDNFTransformer ()) predicate = parser . parse ( statement , on_error = _raise_error ) if isinstance ( predicate , _PyarrowDNFTransformer . Condition ): return [[ predicate . format ()]] if isinstance ( predicate , _PyarrowDNFTransformer . And ): return [ predicate . format ()] if isinstance ( predicate , _PyarrowDNFTransformer . Or ): return predicate . format () raise ValueError ( f \"Invalid statement { statement } \" ) class _PSQLTransformer ( lark . Transformer ): \"\"\"lark.Transformer to translate the grammar into PostgreSQL\"\"\" # pylint: disable=missing-function-docstring,missing-class-docstring,no-self-use def and_operation ( self , operands : lark . Token ): left , right = tuple ( operands ) if \"OR\" in left : left = f \"( { left } )\" if \"OR\" in right : right = f \"( { right } )\" return f \" { left } AND { right } \" def or_operation ( self , operands : lark . Token ): left , right = tuple ( operands ) return f \" { left } OR { right } \" @lark . v_args ( inline = True ) def comparison ( self , left : lark . Token , operator : lark . Token , right : lark . Token ): op = \"<>\" if operator . value == \"!=\" else operator . value return f \" { left } { op } { right } \" @lark . v_args ( inline = True ) def null_comparison ( self , operand : lark . Token , operator : lark . Token ): if operator . type == \"ISNULL\" : return f \" { operand } IS NULL\" if operator . type == \"NOTNULL\" : return f \" { operand } IS NOT NULL\" raise lark . ParseError ( \"Invalid NULL comparison\" ) @lark . v_args ( inline = True ) def negation ( self , expression : lark . Token ): return f \"NOT { expression } \" @lark . v_args ( inline = True ) def notin_list ( self , column : lark . Token , _ : lark . Token , lst : lark . Token ): return f \" { column } NOT IN { lst } \" @lark . v_args ( inline = True ) def in_list ( self , column : lark . Token , lst : lark . Token ): return f \" { column } IN { lst } \" def literal_list ( self , members : lark . Token ): return f \"( { ',' . join ( list ( members )) } )\" def UNQUOTED_COLUMNNAME ( self , name : lark . Token ): return f '\" { name . value } \"' def ESCAPED_STRING ( self , tok : lark . Token ): assert len ( tok ) > 1 and ( tok [ 0 ] == tok [ - 1 ] == \"'\" or tok [ 0 ] == tok [ - 1 ] == '\"' ) return f \"' { tok [ 1 : - 1 ] } '\" def SIGNED_NUMBER ( self , tok : lark . Token ): \"\"\"Convert the value of `tok` from string to number\"\"\" try : return str ( int ( tok )) except ValueError : return str ( float ( tok )) def to_psql ( statement : str ) -> str : DOCS \"\"\"Convert a filter statement to Postgres SQL syntax Args: statement: A filter predicate as string Returns: The filter statement converted to psql. Raises: ValueError: If the statement cannot be parsed Examples: >>> to_psql(\"a.column != 0\") '\"a.column\" <> 0' >>> to_psql(\"a > 1 and b <= 3\") '\"a\" > 1 AND \"b\" <= 3' >>> to_psql(\"a > 1 and b <= 3 or c = 'abc'\") '\"a\" > 1 AND \"b\" <= 3 OR \"c\" = \\\\\\'abc\\\\\\'' \"\"\" parser = _make_parser ( _PSQLTransformer ()) return parser . parse ( statement , on_error = _raise_error )","title":"dframeio.filter"},{"location":"api/source/dframeio/","text":"SOURCE CODE dframeio DOCS \"\"\"Top-level package for dataframe-io.\"\"\" __version__ = \"0.2.0\" # Add Backends one by one if dependencies are available backends = [] try : from dframeio.parquet import ParquetBackend backends . append ( ParquetBackend ) except ModuleNotFoundError as e : if e . name == \"pyarrow\" : pass else : raise try : from dframeio.postgres import PostgresBackend backends . append ( PostgresBackend ) except ModuleNotFoundError as e : if e . name == \"psycopg\" : pass else : raise","title":"dframeio"},{"location":"api/source/dframeio.parquet/","text":"SOURCE CODE dframeio. parquet DOCS \"\"\"Access parquet datasets using pyarrow. \"\"\" import collections.abc import random import re import shutil from pathlib import Path from typing import Dict , Iterable , List , Union import pandas as pd import pyarrow as pa import pyarrow.parquet as pq import dframeio.filter from .abstract import AbstractDataFrameReader , AbstractDataFrameWriter class ParquetBackend ( AbstractDataFrameReader , AbstractDataFrameWriter ): DOCS \"\"\"Backend to read and write parquet datasets Args: base_path: Base path for the parquet files. Only files in this folder or subfolders can be read from or written to. partitions: (For writing only) Columns to use for partitioning. If given, the write functions split the data into a parquet dataset. Subfolders with the following naming schema are created when writing: `column_name=value`. Per default data is written as a single file. Cannot be combined with rows_per_file. rows_per_file: (For writing only) If a positive integer value is given this specifies the desired number of rows per file. The data is then written to multiple files. Per default data is written as a single file. Cannot be combined with partitions. Raises: ValueError: If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. TypeError: If any of the input arguments has a diffent type as documented \"\"\" def __init__ ( self , base_path : str , partitions : Iterable [ str ] = None , rows_per_file : int = 0 ): self . _base_path = base_path if partitions is not None and rows_per_file != 0 : raise ValueError ( \"Only one of 'partitions' and 'rows_per_file' can be used.\" ) if rows_per_file != 0 : if not isinstance ( rows_per_file , int ): raise TypeError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if rows_per_file < 0 : raise ValueError ( f \"Expected a positive integer for rows_per_file, but got { rows_per_file } .\" ) if partitions is not None : if isinstance ( partitions , ( str , bytes )): raise TypeError ( \"partitions must be an integer or an iterable of column names\" ) for _ in partitions : # Raises TypeError if not iterable break self . _partitions = partitions self . _rows_per_file = rows_per_file def read_to_pandas ( DOCS self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> pd . DataFrame : \"\"\"Read a parquet dataset from disk into a pandas DataFrame Args: source: The path of the file or folder with a parquet dataset to read columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows limit: Maximum number of rows to return (limit to first n rows) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows from the final selection Returns: A pandas DataFrame with the requested data. Raises: ValueError: If path specified with `source` is outside of the base path The logic of the filtering arguments is as documented for [`AbstractDataFrameReader.read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" full_path = self . _validated_full_path ( source ) df = self . _read_parquet_table ( full_path , columns = columns , row_filter = row_filter , limit = limit , sample = sample ) if drop_duplicates : return df . to_pandas () . drop_duplicates () return df . to_pandas () def read_to_dict ( DOCS self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> Dict [ str , List ]: \"\"\"Read a parquet dataset from disk into a dictionary of columns Args: source: The path of the file or folder with a parquet dataset to read columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows limit: Maximum number of rows to return (limit to first n rows) sample: Size of a random sample to return drop_duplicates: (Not supported!) Whether to drop duplicate rows Returns: A dictionary with column names as key and a list with column values as values Raises: NotImplementedError: When drop_duplicates is specified The logic of the filtering arguments is as documented for [`AbstractDataFrameReader.read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" if drop_duplicates : raise NotImplementedError ( \"drop_duplicates not available for Parquet -> dict\" ) full_path = self . _validated_full_path ( source ) df = self . _read_parquet_table ( full_path , columns = columns , row_filter = row_filter , limit = limit , sample = sample ) return df . to_pydict () def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): DOCS \"\"\"Write data with full replacement of an existing dataset Args: target: The path of the file or folder to write to. The path may be absolute or relative to the base_path given in the [`__init__()`](dframeio.parquet.ParquetBackend) function. dataframe: The data to write as pandas.DataFrame or as a Python dictionary in the format `column_name: [column_data]` Raises: ValueError: If the dataframe does not contain the columns to partition by as specified in the [`__init__()`](dframeio.parquet.ParquetBackend) function. TypeError: When the dataframe is neither an pandas.DataFrame nor a dictionary \"\"\" full_path = self . _validated_full_path ( target ) if full_path . exists (): if full_path . is_file (): full_path . unlink () elif full_path . is_dir (): shutil . rmtree ( str ( full_path ), ignore_errors = True ) if self . _rows_per_file > 0 : full_path . mkdir ( exist_ok = True ) for i in range ( 0 , self . _n_rows ( dataframe ), self . _rows_per_file ): pq . write_table ( self . _dataframe_slice_as_arrow_table ( dataframe , i , i + self . _rows_per_file ), where = str ( full_path / ( full_path . name + str ( i ))), flavor = \"spark\" , compression = \"snappy\" , ) else : arrow_table = self . _to_arrow_table ( dataframe ) if self . _partitions is not None : missing_columns = set ( self . _partitions ) - set ( arrow_table . column_names ) if missing_columns : raise ValueError ( f \"Expected the dataframe to have the partition columns { missing_columns } \" ) pq . write_to_dataset ( arrow_table , root_path = str ( full_path ), partition_cols = self . _partitions , flavor = \"spark\" , compression = \"snappy\" , ) else : pq . write_table ( arrow_table , where = str ( full_path ), flavor = \"spark\" , compression = \"snappy\" ) def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): DOCS \"\"\"Write data in append-mode\"\"\" full_path = self . _validated_full_path ( target ) if full_path . exists () and full_path . is_file (): if isinstance ( dataframe , pd . DataFrame ): dataframe = pd . concat ([ self . read_to_pandas ( str ( full_path )), dataframe ]) elif isinstance ( dataframe , collections . abc . Mapping ): old_data = self . _read_parquet_table ( str ( full_path ), use_pandas_metadata = False ) . to_pydict () self . _remove_matching_keys ( old_data , r \"__index_level_\\d+__\" ) if set ( old_data . keys ()) != set ( dataframe . keys ()): raise ValueError ( \"Can only append with identical columns. \" f \"Existing columns: { set ( old_data . keys ()) } \" f \"New columns: { set ( dataframe . keys ()) } .\" ) dataframe = { k : old_data [ k ] + dataframe [ k ] for k in old_data . keys ()} else : raise TypeError ( \"dataframe must be either a pandas.DataFrame or a dictionary. \" f \"Got type { type ( dataframe ) } \" ) full_path . unlink () if self . _rows_per_file > 0 : full_path . mkdir ( exist_ok = True ) filename_index = 0 for i in range ( 0 , self . _n_rows ( dataframe ), self . _rows_per_file ): while ( full_path / ( full_path . name + str ( filename_index ))) . exists (): filename_index += 1 pq . write_table ( self . _dataframe_slice_as_arrow_table ( dataframe , i , i + self . _rows_per_file ), where = str ( full_path / ( full_path . name + str ( filename_index ))), flavor = \"spark\" , compression = \"snappy\" , ) filename_index += 1 else : arrow_table = self . _to_arrow_table ( dataframe ) if self . _partitions is not None : missing_columns = set ( self . _partitions ) - set ( arrow_table . column_names ) if missing_columns : raise ValueError ( f \"Expected the dataframe to have the partition columns { missing_columns } \" ) pq . write_to_dataset ( arrow_table , root_path = str ( full_path ), partition_cols = self . _partitions , flavor = \"spark\" , compression = \"snappy\" , ) else : pq . write_table ( arrow_table , where = str ( full_path ), flavor = \"spark\" , compression = \"snappy\" , ) @staticmethod def _remove_matching_keys ( d : collections . abc . Mapping , regex : str ): \"\"\"Remove all keys matching regex from the dictionary d\"\"\" compiled_regex = re . compile ( regex ) keys_to_delete = [ k for k in d . keys () if compiled_regex . match ( k )] for k in keys_to_delete : del d [ k ] @staticmethod def _n_rows ( dataframe : Union [ pd . DataFrame , Dict [ str , List ]]) -> int : \"\"\"Returns the number of rows in the dataframe Returns: Number of rows as int Raises: TypeError: When the dataframe is neither an pandas.DataFrame nor a dictionary \"\"\" if isinstance ( dataframe , pd . DataFrame ): return len ( dataframe ) if isinstance ( dataframe , collections . abc . Mapping ): return len ( next ( iter ( dataframe . values ()))) raise TypeError ( \"dataframe must be a pandas.DataFrame or dict\" ) @staticmethod def _dataframe_slice_as_arrow_table ( dataframe : Union [ pd . DataFrame , Dict [ str , List ]], start : int , stop : int ): if isinstance ( dataframe , pd . DataFrame ): return pa . Table . from_pandas ( dataframe . iloc [ start : stop ], preserve_index = True ) if isinstance ( dataframe , collections . abc . Mapping ): return pa . Table . from_pydict ( { colname : col [ start : stop ] for colname , col in dataframe . items ()} ) raise ValueError ( \"dataframe must be a pandas.DataFrame or dict\" ) def _validated_full_path ( self , path : Union [ str , Path ]) -> Path : \"\"\"Make sure the given path is in self._base_path and return the full path Returns: The full path as pathlib object Raises: ValueError: If the path is not in the base path \"\"\" full_path = Path ( self . _base_path ) / path if Path ( self . _base_path ) not in full_path . parents : raise ValueError ( f \"The given path { path } is not in base_path { self . _base_path } !\" ) return full_path @staticmethod def _to_arrow_table ( dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): \"\"\"Convert the dataframe to an arrow table\"\"\" if isinstance ( dataframe , pd . DataFrame ): return pa . Table . from_pandas ( dataframe , preserve_index = True ) if isinstance ( dataframe , collections . abc . Mapping ): return pa . Table . from_pydict ( dataframe ) raise ValueError ( \"dataframe must be a pandas.DataFrame or dict\" ) @staticmethod def _read_parquet_table ( full_path : Union [ str , Path ], columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , use_pandas_metadata : bool = True , ) -> pa . Table : \"\"\"Read a parquet dataset from disk into a parquet.Table object Args: source: The full path of the file or folder with a parquet dataset to read columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows limit: Maximum number of rows to return (limit to first n rows) use_pandas_metadata: Whether to read also pandas data such as index columns Returns: Content of the file as a pyarrow Table \"\"\" kwargs = dict ( columns = columns , use_threads = True , use_pandas_metadata = use_pandas_metadata ) if row_filter : kwargs [ \"filters\" ] = dframeio . filter . to_pyarrow_dnf ( row_filter ) df = pq . read_table ( str ( full_path ), ** kwargs ) if limit >= 0 : df = df . slice ( 0 , min ( df . num_rows , limit )) if sample >= 0 : indices = random . sample ( range ( df . num_rows ), min ( df . num_rows , sample )) df = df . take ( indices ) return df","title":"dframeio.parquet"},{"location":"api/source/dframeio.postgres/","text":"SOURCE CODE dframeio. postgres DOCS \"\"\"Access PostgreSQL databases using psycopg3. \"\"\" from typing import Dict , List , Union import pandas as pd import psycopg import psycopg.sql as psql from .abstract import AbstractDataFrameReader , AbstractDataFrameWriter from .filter import to_psql class PostgresBackend ( AbstractDataFrameReader , AbstractDataFrameWriter ): DOCS \"\"\"Backend to read and write PostgreSQL tables Args: conninfo: Connection string in libq format. See the [PostgreSQL docs][1] for details. connection_factory: Alternative way of connecting is to provide a function returning an already established connection. Raises: ValueError: If any of the input arguments are outside of the documented value ranges or if conflicting arguments are given. TypeError: If any of the input arguments has a diffent type as documented [1]: https://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING \"\"\" _connection : psycopg . Connection _batch_size : int = 1000 def __init__ ( self , conninfo : str = None , * , autocommit = True ): super () . __init__ () self . _connection = psycopg . connect ( conninfo = conninfo , autocommit = autocommit ) def read_to_pandas ( DOCS self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> pd . DataFrame : \"\"\"Read a postgres table into a pandas DataFrame Args: source: The table name (may include a database name) columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows limit: Maximum number of rows to return (limit to first n rows) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows from the final selection Returns: A pandas DataFrame with the requested data. Raises: ValueError: If path specified with `source` is outside of the base path The logic of the filtering arguments is as documented for [`read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" query = self . _make_psql_query ( source , columns , row_filter , limit , sample , drop_duplicates ) dataframe = pd . read_sql_query ( query , self . _connection ) if drop_duplicates : return dataframe . drop_duplicates () return dataframe def read_to_dict ( DOCS self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> Dict [ str , List ]: \"\"\"Read data into a dict of named columns Args: source: A string specifying the data source (format differs by backend) columns: List of column names to limit the reading to row_filter: NOT IMPLEMENTED. Reserved keyword for filtering rows. limit: Maximum number of rows to return (top-n) sample: Size of a random sample to return drop_duplicates: Whether to drop duplicate rows Returns: A dictionary with column names as key and a list with column values as values The logic of the filtering arguments is as documented for [`read_to_pandas()`](dframeio.abstract.AbstractDataFrameReader.read_to_pandas). \"\"\" query = self . _make_psql_query ( source , columns , row_filter , limit , sample , drop_duplicates ) with self . _connection . cursor () as cursor : cursor . execute ( query ) # Preallocate dataframe as list of lists table = [[ None ] * cursor . rowcount for _ in range ( cursor . pgresult . nfields )] # Fetch the data row_idx = 0 while row_idx < cursor . rowcount : for row_as_tuples in cursor . fetchmany ( size = self . _batch_size ): # if row_as_tuples is not None: for col_idx , cell in enumerate ( row_as_tuples ): table [ col_idx ][ row_idx ] = cell row_idx += 1 return { column . name : table [ i ] for i , column in enumerate ( cursor . description )} def _make_psql_query ( self , source : str , columns : List [ str ] = None , row_filter : str = None , limit : int = - 1 , sample : int = - 1 , drop_duplicates : bool = False , ) -> psql . SQL : \"\"\"Compose a full SQL query from the information given in the arguments. Args: source: The table name (may include a database name) columns: List of column names to limit the reading to row_filter: Filter expression for selecting rows limit: Maximum number of rows to return (limit to first n rows) sample: Size of a random sample to return Returns: Prepared query for psycopg \"\"\" if limit != - 1 and sample != - 1 : sample = min ( limit , sample ) limit = - 1 table = psql . Identifier ( source ) select = psql . SQL ( \"SELECT DISTINCT\" ) if drop_duplicates else psql . SQL ( \"SELECT\" ) columns_clause = self . _make_columns_clause ( columns ) where_clause = self . _make_where_clause ( row_filter ) limit_clause = self . _make_limit_clause ( limit ) sample_clause = self . _make_sample_clause ( sample ) query = psql . SQL ( \" \" ) . join ( [ x for x in [ select , columns_clause , psql . SQL ( \"FROM\" ), table , where_clause , sample_clause , limit_clause , ] if x ] ) return query @staticmethod def _make_where_clause ( row_filter : str = None ) -> psql . SQL : \"\"\"Create a psql WHERE clause from dframeio's standard SQL syntax\"\"\" if row_filter : psql_filter = to_psql ( row_filter ) return psql . SQL ( f \" WHERE { psql_filter } \" ) return psql . SQL ( \"\" ) @staticmethod def _make_columns_clause ( columns : List [ str ]) -> psql . SQL : \"\"\"Create the list of columns for a select statement in psql syntax\"\"\" if isinstance ( columns , str ): raise TypeError ( f \"'columns' must be a list of column names. Got ' { columns } '\" ) return ( psql . SQL ( \",\" ) . join ([ psql . Identifier ( c ) for c in columns ]) if columns else psql . SQL ( \"*\" ) ) @staticmethod def _make_sample_clause ( sample : int ) -> psql . Composed : \"\"\"Create a sample clause in psql syntax if limit != -1, otherwise return empty clause\"\"\" if sample != - 1 : if not isinstance ( sample , int ): raise TypeError ( f \"'limit' must be a positive integer. Got { sample } \" ) return psql . SQL ( \" ORDER BY RANDOM() LIMIT {sample} \" ) . format ( sample = sample ) return psql . Composed ([]) @staticmethod def _make_limit_clause ( limit : int ) -> psql . Composed : \"\"\"Create a limit clause in psql syntax if limit != -1, otherwise return empty clause\"\"\" if limit != - 1 : if not isinstance ( limit , int ): raise TypeError ( f \"'limit' must be a positive integer. Got { limit } \" ) return psql . SQL ( \" LIMIT {limit} \" ) . format ( limit = limit ) return psql . Composed ([]) def write_replace ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): DOCS \"\"\"Write data to a Postgres table after deleting all the existing content Args: target: The database table to write to. dataframe: The data to write as pandas.DataFrame or as a Python dictionary in the format `column_name: [column_data]` Raises: TypeError: When the dataframe is neither a pandas.DataFrame nor a dictionary \"\"\" if not isinstance ( dataframe , ( pd . DataFrame , dict )): raise TypeError ( \"dataframe must either be a pandas DataFrame \" f \"or a dict of lists but was { dataframe } \" ) table = psql . Identifier ( target ) query = psql . SQL ( \"DELETE FROM {table} \" ) . format ( table = table ) with self . _connection . cursor () as cursor : cursor . execute ( query ) self . write_append ( target , dataframe ) def write_append ( self , target : str , dataframe : Union [ pd . DataFrame , Dict [ str , List ]]): DOCS \"\"\"Write data in append-mode to a Postgres table Args: target: The database table to write to. dataframe: The data to write as pandas.DataFrame or as a Python dictionary in the format `column_name: [column_data]` Raises: TypeError: When the dataframe is neither a pandas.DataFrame nor a dictionary \"\"\" table = psql . Identifier ( target ) if isinstance ( dataframe , pd . DataFrame ): columns = psql . SQL ( \",\" ) . join ([ psql . Identifier ( c ) for c in dataframe . columns ]) values = \", \" . join ( len ( dataframe . columns ) * [ \" %s \" ]) query = psql . SQL ( \"INSERT INTO {table} ( {columns} ) VALUES (\" + values + \")\" ) . format ( table = table , columns = columns , values = values ) with self . _connection . cursor () as cursor : cursor . executemany ( query , map ( tuple , dataframe . where ( dataframe . notnull ()) . values )) elif isinstance ( dataframe , dict ): columns = psql . SQL ( \",\" ) . join ([ psql . Identifier ( c ) for c in dataframe ]) values = \", \" . join ( len ( dataframe ) * [ \" %s \" ]) query = psql . SQL ( \"INSERT INTO {table} ( {columns} ) VALUES (\" + values + \")\" ) . format ( table = table , columns = columns , values = values ) with self . _connection . cursor () as cursor : cursor . executemany ( query , zip ( * dataframe . values ())) else : raise TypeError ( \"dataframe must either be a pandas DataFrame or a dict of lists\" )","title":"dframeio.postgres"}]}